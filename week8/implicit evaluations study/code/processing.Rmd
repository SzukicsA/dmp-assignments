---
title: "Evaluations of positive and negative stimuli using the Affective Misattribution Procedure (AMP) and self-reports"
subtitle: "Data processing"
author: "Template: Ian Hussey; content: [Andreas Szukics Ortiz]"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}

knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

```

# Dependencies

```{r}

library(tidyverse)
library(janitor) # for clean_names()
library(stringr)

```

# Get data

```{r}

# demographics
data_demographics_raw <- read_csv("../data/raw/data_demographics_raw.csv") |>
  janitor::clean_names()

# data_demographics_raw_messy <- read_csv("../data/raw/data_demographics_raw_messy.csv", skip = 2) |>
#   janitor::clean_names()

# self report measure
data_selfreport_raw <- read_csv("../data/raw/data_selfreport_raw.csv") |>
  janitor::clean_names()

# affect attribution procedure
data_amp_raw <- read_csv("../data/raw/data_amp_raw.csv") |>
  janitor::clean_names()

```

# Demographics

```{r}

dat_age_gender <- data_demographics_raw |>
  select(subject, date, time, trialcode, response) |>
  pivot_wider(names_from = trialcode,
              values_from = response) |>
  mutate(gender = tolower(gender),
         gender = stringr::str_remove_all(gender, regex("\\W+")), # regex is both very useful and awful to write
         gender = case_when(gender == "female" ~ gender,
                            gender == "male" ~ gender,
                            gender == "nonbinary" ~ gender,
                            gender == "woman" ~ "female",
                            gender == "man" ~ "male",
                            TRUE ~ "other/missing/error"),
         age = case_when(str_detect(age, "^[0-9]+$") ~ age, # if the value is only numbers, keep it. 
                         TRUE ~ "other/missing/error")) 

```

# Exclusions / data quality

## AMP

Assignments week 8: 

Make your additions in the dpm-assignments repo

Make sure you have copied the current code from your fork of my repo (dpm-demonstration) to yours (dp,-assignments, in a "week 8" folder).

Finish what we did in class: score the AMP in processing.Rmd. 

##The AMP score variable should be called amp_score. Here is the verbal description of how it's to be calculated:

    "The AMP was scored in the usual manner. Each participant’s score represented the proportion of prime-congruent ratings (i.e., positive primes that were rated positively, ie “1”; and negative primes that were rated negatively, ie “0), using only trials from the test blocks.

    correct == 0 ~ negative evaluative response

    correct == 1 ~ positive evaluative response"

    - Implement at least three "sanity tests" in a copy of the current codeyour week 8. These simply check whether the logic you implemented to calculate the AMP score is correct. Do these sanity checks immediatly after calucating the AMP score in processing.Rmd.

    - These checks are quite basic tests of what should be the case. Two that I can think of are very simple. One is very slightly more complex, and must be run before you summarize() to create AMP scores. It would assess if your code for assessing prime_congruence is correct.

    Make necessary additions to the processing.Rmd and analysis.Rmd files and make descriptive statistics for amp_score in analysis.Rmd, just as you did for the self reported evaluations. As the response to this assignment, reply with the mean AMP score in women, taken from your AMP descriptive statistics table. 

    Read about the basics of ggplot

    Most weeks there aren't assigned readings for this course, but this week there is. It's important you read this. You can read either the preprint (shorter but no working code) or the book (working code and interactive exercises). If you read the book, sections 1, 2, and 4 are most relevant.

    https://debruine.github.io/publication/dataviz/

##Please make sure you come to class next week knowing what is meant by the following ggplot terms:

    - layers

    - aesthetics

    - themes

    - facets

You should practice the basics of ggplot using Lisa's code in her book.

Next week, we will do more problem based exercises involving making exploratory plots.


```{r}

data_amp_performance_criteria <- data_amp_raw |> 
  filter(blockcode != "practice", 
         trialcode != "instructions") |> 
  mutate(latency_prob = if_else(latency < 100, TRUE, FALSE)) |> 
  group_by(subject) |> 
  summarize(proportion_fast_trials_amp = mean(latency_prob)) |>
  mutate(exclude_amp_performance = ifelse(proportion_fast_trials_amp > 0.10, "exclude", "include"))

# determine modal number of trials
data_amp_completeness <- data_amp_raw |>
  filter(blockcode != "practice",
         trialcode != "instructions") |>
  group_by(subject) |>
  count() |>
  ungroup() |>
  mutate(exclude_amp_completeness = ifelse(n == 72, "include", "exclude")) |>
  select(-n)

# data_amp_completeness |>
#   count()

```

- One participant with 8 trials appears to be a partial completion (check raw data?)
- One participant with 144 trials appears to be a repeat participant. I've chosen to exclude them entirely, but you could also have a more elaborate strategy where you retain only their first completion.

# Self-reports

```{r}

# trial level data
data_selfreport_trial_level <- data_selfreport_raw |>
  select(subject, trialcode, response) |>
  filter(trialcode %in% c("like", "prefer", "positive")) |>
  rename(item = trialcode) |>
  filter(response != "Ctrl+'B'") |>
  mutate(response = as.numeric(response))

# mean scored
data_selfreport_mean_score <- data_selfreport_trial_level |>
  group_by(subject) |>
  summarize(mean_evaluation = mean(response, na.rm = TRUE))

# combined
data_selfreport_scored <- 
  full_join(data_selfreport_trial_level |>
              pivot_wider(names_from = "item",
                          values_from = "response"),
            data_selfreport_mean_score,
            by = "subject")

```

# Affect Misattribution Procedure

TODO extract evaluations on the AMP test blocks and convert to an overall bias score

```{r}

data_amp_con <- data_amp_raw |>
  filter(blockcode != "practice",
         trialcode != "instructions") |>
   select(-c(date, time, blockcode, blocknum_and_trialnum, primestim, targetstim, latency)) |>
  rename(evaluative_response = correct) |>
  mutate(congruency = case_when(
    trialcode == "prime_positive" & evaluative_response == 1 ~   1,
    trialcode == "prime_negative" & evaluative_response == 0 ~   1,
    TRUE ~ 0))

amp_score <- data_amp_con |>
  group_by(subject) |>
  summarize(mean_congruency = mean(congruency))


# Check if the AMP Score Calculation was Successful
  if (any(is.na(amp_score$count_congruent))) {
  stop("AMP score calculation may have failed.")
  }

# Check for Missing Values in 'congruency'
if (any(is.na(data_amp_con$congruency))) {
  stop("There are missing values in the 'congruency' column.")
}

# Check if 'subject' is Unique
if (any(duplicated(amp_score$subject))) {
  stop("Subject IDs should be unique in the 'amp_score' dataframe.")
}

## Checking whether all subjects have 72 rows
subject_row_counts <- data_amp_con %>%
  group_by(subject) %>%
  summarize(row_count = n())

## Display the row counts for each subject
print(subject_row_counts)

      # - the subject 246532124 has duplicate rows & 504546409 has too few rows

```

# Combine

```{r}

# combine all dfs created in the previous chunks
data_processed_temp <- dat_age_gender |>
  full_join(data_selfreport_scored, by = "subject") |> 
  full_join(data_amp_performance_criteria, by = "subject") |>
  full_join(data_amp_completeness, by = "subject") |> 
  full_join(amp_score, by = "subject")

# flag all subjects with more than one row in the wide-format data. these should be excluded in the analysis.
# a more elaborate approach would be to track down the individual dupicate cases and determine which of the mulitiple cases should be retained. 
data_processed_duplicates <- data_processed_temp |>
  count(subject) |>
  mutate(exclude_duplicate_data = if_else(n > 1, "exclude", "include")) |>
  select(-n)

# join in the duplicates df
data_processed_before_exclusions <- data_processed_temp |>
  full_join(data_processed_duplicates, by = "subject")

```

# Define master exclusions

```{r}

# create a master exclude_participant variable
data_processed <- data_processed_before_exclusions |>
  mutate(exclude_participant = case_when(tolower(age) == "test" ~ "exclude",
                                         tolower(gender) == "test" ~ "exclude",
                                         is.na(mean_evaluation) ~ "exclude",
                                         # in this case we will exclude participants with missing demographics data or outcomes measures data. 
                                         # Note that "list-wise exclusions" like this aren't always justified, as missingness often isn't at random. 
                                         # How to treat missing data is a  whole area of work in itself, which we wont cover here.
                                         is.na(age) ~ "exclude", 
                                         is.na(gender) ~ "exclude",
                                         exclude_amp_performance == "exclude" ~ "exclude",
                                         exclude_duplicate_data == "exclude" ~ "exclude",
                                         exclude_amp_completeness == "exclude" ~ "exclude", 
                                         TRUE ~ "include"))

```

# Write to disk

```{r}

# in case this dir doesn't exist, create it
dir.create("../data/processed/")

# save data to disk in that dir
write_csv(data_processed, "../data/processed/data_processed.csv")

```

# Session info

```{r}

sessionInfo()

```


