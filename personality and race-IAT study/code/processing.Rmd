---
title: "Examining the relationship between the big-5 personality facets and implicit racial attitudes"
subtitle: "Data processing"
author: "Andreas Szukics"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
    css: styles.css  # CSS file for costumized html
---

```{r, include=FALSE}

knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

```

# Libraries

```{r}

library(tidyverse)
library(knitr)
library(kableExtra)
library(janitor) # for clean_names()
library(stringr)
library(openxlsx)

```

# Loading data

```{r}

# Demographics
data_dem_raw <- read_csv("../data/raw/data_raw_demographics.csv") |>
  janitor::clean_names()


# BFI
data_bfi_raw <- read_csv("../data/raw/data_raw_bfi.csv") |>
  janitor::clean_names()


# IAT
data_iat_raw <- read_csv("../data/raw/data_raw_iat.csv") |>
  janitor::clean_names()


# view(data_dem_raw)
# view(data_bfi_raw)
# view(data_iat_raw)

```

# Demographics

```{r}

map(data_dem_raw, class)

dat_age_gen <- data_dem_raw        |>
  pivot_wider(names_from  = variable,
              values_from = response) |> 
  rename(subject          = unique_id,
          gender          = sex)      |>
  mutate(gender           = as.character(gender), # case_when does not work otherwise (classed as a list after bivor because of NAs)
         age              = as.character(age))


table(dat_age_gen$gender, useNA = "ifany") |>  #checking the values in gender and act accordingly
  kable()  |>
  kable_classic(full_width = FALSE)


dat_age_gen <- dat_age_gen         |>
  mutate(gender           = as.character(gender), # case_when does not work otherwise (classed as a list after bivor because of NAs)
         age              = as.character(age),    # case_when does not work otherwise (classed as a list after bivor because of NAs)
         gender           = tolower(gender),
         gender           = stringr::str_remove_all(gender, regex("\\W+")),
         # checking for NAs and renaming them and changing f/m to female/male
         gender           = case_when(gender == "f" ~ "female",  
                                      gender == "m" ~ "male",   
                                      TRUE ~ "other/missing/error"),
         age              = case_when(str_detect(age, "^[0-9]+$") ~ age,
                                      TRUE ~ "other/missing/error"),
         subject          = case_when(is.na(subject) ~ "other/missing/error",
                                      TRUE ~ as.character(subject)) # there is one missing subject nr
         )



map(dat_age_gen, class) # checking the class for age and gender
```

# Data processing

## BFI:

### Reverse scaling

```{r}

data_bfi_rs <- data_bfi_raw |> 
  rename(subject = unique_id) |> 
  mutate_at(vars(bfi_a1, bfi_a3, bfi_a6, bfi_a8,
                 bfi_c2, bfi_c4, bfi_c5, bfi_c9,
                 bfi_e2, bfi_e5, bfi_e7,
                 bfi_n2, bfi_n5, bfi_n7,
                 bfi_o7, bfi_o9), 
            ~ ifelse(!is.na(.), 7 - ., NA))  # this is to ignore missing values

```


### Sanity check for reversal

```{r}

# selected_columns_a <- paste0("bfi_a", 1:9)
# selected_columns_a <- paste0("bfi_c", 1:9)
# selected_columns_a <- paste0("bfi_e", 1:8)
# selected_columns_a <- paste0("bfi_n", 1:8)
selected_columns_a <- paste0("bfi_o", 1:10)

# Select the columns for the "bfi_a" subscale
dat_bfi_a <- data_bfi_rs %>%
  select(selected_columns_a)

# Compute the correlation matrix for "bfi_a"
cor_matrix_a <- cor(dat_bfi_a, use = "pairwise.complete.obs")

# Convert the correlation matrix to a data frame
cor_df_a <- as.data.frame(cor_matrix_a)

# Print the correlation data frame for "bfi_a"
print(cor_df_a)
```


```{r}
# List of subscales
subscales <- c("a", "c", "e", "n", "o")
num_items <- c(9, 9, 8, 8, 10)

# Create an empty list to store correlation matrices
cor_matrices <- list()

# Nested loops to compute correlations for each subscale
for (i in seq_along(subscales)) {
  # Generate column names for the current subscale
  selected_columns <- paste0("bfi_", subscales[i], 1:num_items[i])
  
  # Select columns for the current subscale
  dat_subscale <- data_bfi_rs %>% select(selected_columns)
  
  # Compute correlation matrix
  cor_matrix <- cor(dat_subscale, use = "pairwise.complete.obs")
  
  # Store the correlation matrix in the list
  cor_matrices[[subscales[i]]] <- cor_matrix
}

# Print or use the correlation matrices as needed
print(cor_matrices)

```
### Completeness check

```{r}

# List of subscales
subscales <- c("a", "c", "e", "n", "o")
num_items <- c(9, 9, 8, 8, 10)

# Create an empty list to store completeness information
completeness_info <- list()

# Nested loops to check completeness for each subscale
for (i in seq_along(subscales)) {
  # Generate column names for the current subscale
  selected_columns <- paste0("bfi_", subscales[i], 1:num_items[i])
  
  # Select columns for the current subscale
  dat_subscale <- data_bfi_rs %>% select(subject, selected_columns)
  
  # Check if subscale has been started
  subscale_started <- rowSums(!is.na(dat_subscale[, -1])) > 0
  
  # Check if subscale has been completed
  subscale_completed <- rowSums(!is.na(dat_subscale[, -1])) == num_items[i]
  
  # Mark subjects who started but didn't complete the subscale
  subscale_started_but_not_completed <- subscale_started & !subscale_completed
  
  # Store completeness information in the list
  completeness_info[[subscales[i]]] <- data.frame(subject = dat_subscale$subject, 
                                                   started = subscale_started,
                                                   incomplete = subscale_started_but_not_completed)
}

# Combine completeness information for all subscales
completeness_combined <- Reduce(function(x, y) merge(x, y, by = "subject", all = TRUE), completeness_info)

# Check if at least 2 subscales are complete for each subject
completeness_combined$at_least_2_complete <- rowSums(completeness_combined[, -1], na.rm = TRUE) >= 2

# Check if there are no subscales that have been started and are incomplete
no_started_incomplete_subscale <- rowSums(completeness_combined[, grep("incomplete", names(completeness_combined))], na.rm = TRUE) == 0

# Create a new column "completeness_check" in data_bfi_rs
data_bfi_rs$completeness_check <- ifelse(data_bfi_rs$subject %in% completeness_combined$subject[completeness_combined$at_least_2_complete & no_started_incomplete_subscale], "include", "exclude")


# Print or use the data_bfi_rs with the new completeness_check column as needed
view(data_bfi_rs)


```

```{r}

# List of subscales
subscales <- c("a", "c", "e", "n", "o")
num_items <- c(9, 9, 8, 8, 10)

# Create an empty list to store completeness information
completeness_info <- list()

# Get the subject column
subject_column <- data_bfi_rs$subject

# Nested loops to check completeness for each subscale
for (i in seq_along(subscales)) {
  # Generate column names for the current subscale
  selected_columns <- paste0("bfi_", subscales[i], 1:num_items[i])
  
  # Select columns for the current subscale
  dat_subscale <- data_bfi_rs %>% select(subject, selected_columns)
  
  # Check if subscale has been started
  subscale_started <- rowSums(!is.na(dat_subscale[, -1])) > 0
  
  # Check if subscale has been completed
  subscale_completed <- rowSums(!is.na(dat_subscale[, -1])) == num_items[i]
  
  # Mark subjects who started but didn't complete the subscale
  subscale_started_but_not_completed <- subscale_started & !subscale_completed
  
  # Check if values in completed subscales are within 1 and 6 (only for started subscales)
  subscale_values_within_range <- apply(dat_subscale[subscale_started, -1], 1, function(row) all(row >= 1 & row <= 6))
  
  # Store completeness information in the list
  completeness_info[[subscales[i]]] <- data.frame(subject = subject_column,
                                                   started = subscale_started,
                                                   incomplete = subscale_started_but_not_completed,
                                                   values_within_range = subscale_values_within_range)
}

# Combine completeness information for all subscales
completeness_combined <- Reduce(function(x, y) merge(x, y, by = "subject", all = TRUE), completeness_info)

# Check if at least 2 subscales are complete for each subject
completeness_combined$at_least_2_complete <- rowSums(completeness_combined[, -1], na.rm = TRUE) >= 2

# Check if there are no subscales that have been started and are incomplete
no_started_incomplete_subscale <- rowSums(completeness_combined[, grep("incomplete", names(completeness_combined))], na.rm = TRUE) == 0

# Check if all values in started subscales are within 1 and 6
values_within_range <- rowSums(completeness_combined[, grep("values_within_range", names(completeness_combined))], na.rm = TRUE) == length(subscales)

# Create a new column "completeness_check" in data_bfi_rs
data_bfi_rs$completeness_check <- ifelse(data_bfi_rs$subject %in% completeness_combined$subject[completeness_combined$at_least_2_complete & no_started_incomplete_subscale & values_within_range], "include", "exclude")

# Print or use the data_bfi_rs with the new completeness_check column as needed
view(data_bfi_rs)



```




## IAT

```{r}

map(data_iat_raw, class)

data_amp_performance_criteria <- data_amp_raw |> 
  filter(blockcode != "practice", 
         trialcode != "instructions") |> 
  mutate(latency_prob = if_else(latency < 100, TRUE, FALSE)) |> 
  group_by(subject) |> 
  summarize(proportion_fast_trials_amp = mean(latency_prob)) |>
  mutate(exclude_amp_performance = ifelse(proportion_fast_trials_amp > 0.10, "exclude", "include"))

# determine modal number of trials
data_amp_completeness <- data_amp_raw |>
  filter(blockcode != "practice",
         trialcode != "instructions") |>
  group_by(subject) |>
  count() |>
  ungroup() |>
  mutate(exclude_amp_completeness = ifelse(n == 72, "include", "exclude")) |>
  select(-n)

# data_amp_completeness |>
#   count(n)

```

- One participant with 8 trials appears to be a partial completion (check raw data?)
- One participant with 144 trials appears to be a repeat participant. I've chosen to exclude them entirely, but you could also have a more elaborate strategy where you retain only their first completion.

# Self-reports

```{r}

# trial level data
data_selfreport_trial_level <- data_selfreport_raw |>
  select(subject, trialcode, response) |>
  filter(trialcode %in% c("like", "prefer", "positive")) |>
  rename(item = trialcode) |>
  filter(response != "Ctrl+'B'") |>
  mutate(response = as.numeric(response))

# mean scored
data_selfreport_mean_score <- data_selfreport_trial_level |>
  group_by(subject) |>
  summarize(mean_evaluation = mean(response, na.rm = TRUE))

# combined
data_selfreport_scored <- 
  full_join(data_selfreport_trial_level |>
              pivot_wider(names_from = "item",
                          values_from = "response"),
            data_selfreport_mean_score,
            by = "subject")

```

# Affect Misattribution Procedure

TODO extract evaluations on the AMP test blocks and convert to an overall bias score

```{r}

data_amp_score_congruence <- data_amp_raw |> 
  select(subject, evaluative_response = correct, trialcode, blockcode) |> 
  filter(blockcode != "practice", 
         trialcode != "instructions") |> 
  mutate(trialcode = case_when(trialcode == "prime_positive" ~ 1, 
                               trialcode == "prime_negative" ~ 0,
                               TRUE ~ NA),
         prime_congruence = ifelse(trialcode == evaluative_response, 1, 0)) 

# sanity check 1: if you consider all the combiantions of factor levels of trialcode, evaluative_response, and prime congruence, there should be only 4:
data_amp_score_congruence |>
  count(trialcode, evaluative_response, prime_congruence)

data_amp_score_congruence |>
  count(trialcode, evaluative_response, prime_congruence) |>
  nrow() == 4

# calculate AMP score 
data_amp_score <- data_amp_score_congruence |> 
  group_by(subject) |> 
  summarize(AMP_score = mean(prime_congruence, na.rm = TRUE)) |> 
  select(subject, AMP_score)

# sanity check 2: check if AMP_score is numeric 
is.numeric(data_amp_score$AMP_score)

# sanity check 3: check if AMP_score is bounded [0,1]
data_amp_score |> 
  mutate(bounded_correctly = between(AMP_score, left = 0, right = 1)) |>
  filter(bounded_correctly != TRUE) |>
  nrow() == 0

```

# Combine

```{r}

# combine all dfs created in the previous chunks
data_processed_temp <- dat_age_gender |>
  full_join(data_selfreport_scored, by = "subject") |> 
  full_join(data_amp_score, by = "subject") |> 
  full_join(data_amp_performance_criteria, by = "subject") |>
  full_join(data_amp_completeness, by = "subject")

# flag all subjects with more than one row in the wide-format data. these should be excluded in the analysis.
# a more elaborate approach would be to track down the individual dupicate cases and determine which of the mulitiple cases should be retained. 
data_processed_duplicates <- data_processed_temp |>
  count(subject) |>
  mutate(exclude_duplicate_data = if_else(n > 1, "exclude", "include")) |>
  select(-n)

# join in the duplicates df
data_processed_before_exclusions <- data_processed_temp |>
  full_join(data_processed_duplicates, by = "subject")

```

# Define master exclusions

```{r}

# create a master exclude_participant variable
data_processed <- data_processed_before_exclusions |>
  mutate(exclude_participant = case_when(tolower(age) == "test" ~ "exclude",
                                         tolower(gender) == "test" ~ "exclude",
                                         is.na(mean_evaluation) ~ "exclude",
                                         # in this case we will exclude participants with missing demographics data or outcomes measures data. 
                                         # Note that "list-wise exclusions" like this aren't always justified, as missingness often isn't at random. 
                                         # How to treat missing data is a  whole area of work in itself, which we wont cover here.
                                         is.na(age) ~ "exclude", 
                                         is.na(gender) ~ "exclude",
                                         exclude_amp_performance == "exclude" ~ "exclude",
                                         exclude_duplicate_data == "exclude" ~ "exclude",
                                         exclude_amp_completeness == "exclude" ~ "exclude", 
                                         TRUE ~ "include"))

```

# Write to disk

```{r}

# in case this dir doesn't exist, create it
dir.create("../data/processed/")

# save data to disk in that dir
write_csv(data_processed, "../data/processed/data_processed.csv")

```

# Create codebook template for the processed data

If it has not already been created, this code write the codebook template to disk. 

\TODO The template should then be filled in manually with descriptions of each variable so that someone else could understand what these variables represent. 

```{r}

if(!file.exists("../data/communications/data_processed_codebook.xlsx")){
  # convert the column names to a df
  codebook_template <- data.frame(variable = colnames(data_processed)) |>
    mutate(explanation = NA)
  # write to disk as an excel file
  write.xlsx(codebook_template, file = "../data/communications/data_processed_codebook.xlsx")
}

```

Note that there are other ways of automatically creating more elaborate codebooks from your datasets. These often contain information about min/max/mean/SD, distribution, etc. For example:

- Ruben Arslan's {codebook}
  - [R package with How-Tos](https://rubenarslan.github.io/codebook/)
  - [Tutorial](https://rubenarslan.github.io/codebook/articles/codebook_tutorial.html)
  - [Article](https://journals.sagepub.com/doi/full/10.1177/2515245919838783)
  
- Petersen & Ekstrøm's {dataReporter}
  - [Article](https://www.jstatsoft.org/article/view/v090i06)
  - [Blog](https://sandsynligvis.dk/2017/08/21/datamaid-your-personal-assistant-for-cleaning-up-the-data-cleaning-process/)
  - [R package](https://cran.r-project.org/web/packages/dataReporter/index.html)

# Session info

```{r}

sessionInfo()

```





